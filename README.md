# ğŸ¯ EvelutorApp â€” Fair & Transparent Judging Platform

> A modern web platform for managing multi-round evaluation events like hackathons, idea pitches, and innovation challenges â€” with built-in bias correction and real-time results.

---

## ğŸŒŸ Overview

**EvelutorApp** streamlines event evaluation from setup to final results.  
Admins can create events, assign judges, manage rounds, and automatically normalize scores to ensure fairness â€” eliminating judge bias using **Z-score normalization**.

This project aims to make competition scoring **transparent**, **data-driven**, and **simple**.

---

## ğŸ§© Core Features

### ğŸ‘©â€ğŸ’¼ Admin Portal
- Create and configure events, rounds, venues, and categories  
- Define weighted scoring categories (e.g., *Innovation*, *Technical*, *Presentation*)  
- Manage teams, judges, and assignments  
- Trigger computations: **Top-N selection**, **Normalization**, **Final Rankings**  
- Export results (CSV/PDF)  
- View and override scores with full audit trail  

### ğŸ§‘â€âš–ï¸ Judge Interface
- Simple, distraction-free scoring form  
- Enter category scores via secure tokenized links (no login hassle)  
- Auto-calculated totals and validation  

### ğŸ“Š Results & Fairness Engine
- Judge-wise **Z-score normalization** to remove leniency/harshness bias  
- Percentile-based ranking for final results  
- Multi-round progression (Round 1 â†’ Round 2 finalists)  
- Supports manual edits, audit logging, and data exports  

---

## ğŸ§  How It Works

### ğŸ§® Scoring Pipeline

1. **Raw Totals** â†’ judges rate teams across categories  
2. **Z-score Normalization** â†’ adjusts for judge bias  
3. **Aggregation** â†’ team mean Z computed across judges  
4. **Percentile Mapping** â†’ converts to fair, intuitive scores  
5. **Ranking** â†’ final team order is derived

This ensures results reflect *relative performance*, not judge variability.

---

## ğŸ—ƒï¸ Tech Stack

| Layer | Technology |
|-------|-------------|
| **Frontend** | React + Tailwind CSS |
| **Backend** | Node.js / Express *(or Django/Flask alternative)* |
| **Database** | PostgreSQL |
| **Auth** | JWT for Admin, Secure Token Links for Judges |
| **Hosting** | Vercel / AWS / DigitalOcean |
| **Optional** | WebSockets (Live dashboards) |

---

## âš™ï¸ API Structure (Simplified)

| Endpoint | Method | Description |
|-----------|---------|-------------|
| `/api/events` | POST | Create new event |
| `/api/events/:id/categories` | POST | Add category |
| `/api/events/:id/judges` | POST | Add judge |
| `/api/rounds/:id/evaluations` | POST | Submit scores |
| `/api/rounds/:id/normalize` | POST | Run normalization |
| `/api/rounds/:id/results` | GET | View final results |
| `/api/events/:id/export` | GET | Export CSV/PDF |

---

## ğŸ’» Frontend Modules

- **AdminDashboard** â€” manage events, judges, and rounds  
- **CategoryManager** â€” create/edit weighted categories  
- **AssignmentPage** â€” assign judges to teams/venues  
- **JudgeInterface** â€” intuitive scoring page (mobile-friendly)  
- **LiveResultsPage** â€” view normalized rankings  
- **RoundManagementPage** â€” compute results and advance rounds  

---

## ğŸ“ Normalization Algorithm (Simplified JS)

```js
z = (score - meanJudgeScore) / stdDeviationJudge
teamZ = average(zScoresFromAllJudges)
percentile = rank(teamZ) / (totalTeams - 1)
